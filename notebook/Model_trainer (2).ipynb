{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75821847-ebe9-4fd1-a3c8-8367fb791ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_file_path = \"/FileStore/tables/gold_train.parquet\"\n",
    "test_file_path = \"/FileStore/tables/gold_test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad5be9b-f6ba-41de-a1d6-68333754a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(train_file_path)\n",
    "test = spark.read.format(\"parquet\").load(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1363f191-0125-49bd-9370-949d44bf1805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,  MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                      labelCol='Attrition_index')\n",
    "\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5366ab-41a4-4bc2-9d27-689ab2cba699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+----------+--------------------+\n|            features|Attrition|Attrition_index|prediction|         probability|\n+--------------------+---------+---------------+----------+--------------------+\n|(26,[0,1,2,3,4,5,...|   Stayed|            0.0|       0.0|[0.64530855809774...|\n|(26,[0,1,2,3,4,5,...|   Stayed|            0.0|       0.0|[0.93547049474102...|\n|(26,[0,1,2,3,4,5,...|     Left|            1.0|       1.0|[0.43991746387523...|\n|[-1.6830448420997...|   Stayed|            0.0|       1.0|[0.40473876303161...|\n|[-0.8578298986642...|   Stayed|            0.0|       0.0|[0.91825702865186...|\n+--------------------+---------+---------------+----------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictions = lr_model.transform(test)\n",
    "predictions.select(\"features\",\"Attrition\", 'Attrition_index', \"prediction\", \"probability\").show(5)\n",
    "\n",
    "# Evaluation\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Attrition_index\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Initialize MulticlassClassificationEvaluator for metrics like Accuracy, Precision, Recall\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"Attrition_index\", predictionCol=\"prediction\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd082438-216c-4741-9afa-0c3de9ce69d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC (AUC): 0.7260020533002525\nAccuracy: 0.7268926056338029\n"
     ]
    }
   ],
   "source": [
    "# Calculate Area Under ROC (AUC)\n",
    "auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"Area Under ROC (AUC): {auc}\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4416963-0e56-4840-a69a-1d03d50009a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier# checking for features importance \n",
    "\n",
    "# Define the RandomForest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Attrition_index\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ea0171-a683-4906-a851-0060167e8e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n|             Feature|  Importance|\n+--------------------+------------+\n|     Overtime_onehot|  0.30360973|\n|Company Reputatio...| 0.061705243|\n|Performance Ratin...|   0.0381825|\n|     Job Role_onehot| 0.022714224|\n|Education Level_i...| 0.021211961|\n|Innovation Opport...| 0.016206777|\n|Work-Life Balance...| 0.008545652|\n|  Remote Work_onehot| 0.007681143|\n|     Job Level_index|0.0069579277|\n|scaled_numerical_...|0.0058900337|\n|Employee Recognit...|  0.00501718|\n|Job Satisfaction_...| 0.004442644|\n|       Gender_onehot|0.0035444451|\n|  Company Size_index|0.0025570716|\n|Leadership Opport...|0.0010700992|\n|Marital Status_on...|5.1556166E-5|\n+--------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "feature_columns = ['scaled_numerical_feature_vector',\n",
    " 'Work-Life Balance_index',\n",
    " 'Job Satisfaction_index',\n",
    " 'Performance Rating_index',\n",
    " 'Education Level_index',\n",
    " 'Job Level_index',\n",
    " 'Company Size_index',\n",
    " 'Company Reputation_index',\n",
    " 'Employee Recognition_index',\n",
    " 'Gender_onehot',\n",
    " 'Job Role_onehot',\n",
    " 'Overtime_onehot',\n",
    " 'Marital Status_onehot',\n",
    " 'Remote Work_onehot',\n",
    " 'Leadership Opportunities_onehot',\n",
    " 'Innovation Opportunities_onehot']\n",
    "# Convert importances to a list of tuples with each feature name and its importance as float\n",
    "importances_list = [(name, float(importance)) for name, importance in zip(feature_columns, importances)]\n",
    "\n",
    "# Create the DataFrame with explicit schema definition\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "schema = StructType([\n",
    "    StructField(\"Feature\", StringType(), True),\n",
    "    StructField(\"Importance\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with the defined schema\n",
    "feature_importance_df = spark.createDataFrame(importances_list, schema=schema)\n",
    "\n",
    "# Show feature importance sorted by descending order\n",
    "feature_importance_df.orderBy(\"Importance\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e159402e-3fc7-4bdd-8eff-7eb9553b9be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model_trainer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
